# LLM 参数传递问题调试报告

## 问题背景

用户报告 LLM 在回复时表现异常，生成的回复过于简短，不符合设置的预期。经过系统化调试，发现了几个关键问题点。

## 发现的问题

### 1. maxTokens 参数传递问题

1. **命名不一致**：前端和内部使用`maxTokens`，但 OpenAI API 需要`max_tokens`
2. **类型转换问题**：在转换过程中，`null`、`0`等特殊值处理不一致
3. **验证不完善**：没有对无效值(如负数、非数字字符串)进行适当处理
4. **日志缺失**：关键环节缺少详细日志，难以排查问题

### 2. systemPrompt 参数问题

1. **传递链不完整**：虽然前端正确发送，但后端处理中间环节可能丢失
2. **字段检查不充分**：未对空字符串、无效值等进行充分验证
3. **日志记录不足**：没有明确记录是否成功应用自定义系统提示

### 3. API 调用问题

1. **参数转换**：在最终 API 调用前未进行最后的格式检查和转换
2. **错误处理不完善**：API 错误未提供足够上下文信息来定位问题
3. **默认值处理**：对于未设置值时的默认行为缺乏明确文档

## 实施的修复

1. **增强参数验证**：

   - 在`formatAiSettings`函数中添加全面的类型检查和错误处理
   - 对各种边缘情况(如 null、负值、无效字符串)进行特殊处理
   - 添加详细的日志记录，跟踪参数在各转换阶段的变化

2. **改进 buildApiOptions 函数**：

   - 确保字段命名一致性，正确从`maxTokens`映射到`max_tokens`
   - 添加调试日志，记录参数转换过程

3. **增强 aiUtils.js 中的处理**：

   - 对 API 请求参数进行深度克隆，避免意外修改
   - 添加额外的类型检查和日志
   - 记录 API 响应的详细信息，包括 token 使用情况

4. **添加系统化测试工具**：
   - 创建`test-param-tracing-mini.js`进行参数传递测试
   - 创建`test-api-error.js`测试 API 连接和错误分析
   - 设计全面的测试用例验证各类参数值

## 结论

LLM 回复简短的主要原因是参数传递链中的不一致性导致 maxTokens 参数未正确传递到 API 调用。随着我们实施的改进，系统现在能够:

1. 正确处理并传递所有类型的 maxTokens 值
2. 确保 systemPrompt 正确应用
3. 提供充分的日志记录以便问题诊断
4. 在出现问题时提供更明确的错误信息

这些改进应该能有效解决 LLM 响应异常简短的问题，同时为未来的调试提供更多工具和数据点。
